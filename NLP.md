# Natural Language Processing (NLP, обработка естественного языка)
## Bag of Words (“Мешок слов”)
Модель, позволяющая подсчитывать все слова в фрагменте текста. Cоздает матрицу вхождений для предложения или целого документа, игнорируя грамматику и порядок слов. Эта частота появления или вхождения слов затем используются в качестве признаков для обучения классификатора.

**Недостатки:**

Не учитывается семантическое значение и контекст, игнорируемые в иных методах слова (например, “the” или “a”) добавляют нежелательный шум в анализ, и некоторые слова имеют не совсем адекватные веса. Один из способов сгладить проблему - использование TF-IDF. 

## Term Frequency — Inverse Document Frequency” (TF-IDF, Частота терминов — обратная частота документа)
Подход, который, который перемасштабирует частоту слов по частоте их появления во всех текстах (а не только в том, который мы в данный момент анализируем), чтобы веса часто встречающихся слов, таких как “the”, также часто встречаются и в других текстах, получали определенный штраф. С помощью TFIDF слова, которые часто встречаются в тексте, “вознаграждаются”, но они также “штрафуются” на основе того, насколько часто они встречаются в других текстах, которые мы также учитываем в алгоритме. И наоборот, этот метод выделяет и “вознаграждает” слова уникальные или редкие, с учетом всех текстов. Тем не менее, этот подход по-прежнему не имеет ни контекста, ни семантики.

## Tokenization (Токенизация)
Процесс сегментации текстового набора на слова и предложения. По сути, его задача заключается в разделении текста на части, называемые токенами, и отбрасывании определенных символов, например, знаков препинания.

**Недостатки:**

Подходит не для всех языков. Возможно нежелательное разбиение на токены, например, слово может разбиться на 2 токена, как в случае с некоторыми именами собственными (Сан-Франциско, Нью-Йорк) или заимствованными иностранными фразами (например, laissez faire). Токенизация может удалить знаки препинания, что вызывает проблемы с такими словами, как "dr.".

```python
# Токенизация текста на предложения

text_ru = """Граф Лев Николаевич Толсто́й[К 1] (28 августа [9 сентября] 1828, Ясная Поляна, Тульская губерния, Российская 
империя — 7 [20] ноября 1910, станция Астапово, Рязанская губерния, Российская империя) — один из наиболее известных русских 
писателей и мыслителей, один из величайших писателей-романистов мира[4]. Участник обороны Севастополя. Просветитель, публицист, 
религиозный мыслитель, его авторитетное мнение послужило причиной возникновения нового религиозно-нравственного течения — 
толстовства. За свои взгляды был отлучен от церкви. Член-корреспондент Императорской Академии наук (1873), почётный академик 
по разряду изящной словесности (1900)[5]. Был номинирован на Нобелевскую премию по литературе (1902, 1903, 1904, 1905). 
Впоследствии отказался от дальнейшей номинации.
"""

import nltk
from nltk.tokenize import sent_tokenize
tokens = sent_tokenize(text_ru)
print(f'{tokens[0]}\n\n{tokens[1]}\n\n{tokens[2]}')
```
```python
# Токенизация текста на слова

from nltk.tokenize import word_tokenize
token_word = word_tokenize(text_ru)
print(token_word)
```
```python
# Поиск 10 наиболее распространенных слов

from nltk.probability import FreqDist
dist = FreqDist(token_word)
dist.most_common(10)
```
Для русского текста лучше использовать библиотеку razdel
```python
pip install razdel

from razdel import sentenize, tokenize

text_generator = sentenize(text_ru)
print(next(text_generator))
print(next(text_generator))
print(next(text_generator))

list(tokenize(text_ru))[:10]
```

## Stop Words Removal (Удаление шумовых слов)
Избавление от общеупотребительных артиклей, местоимений и предлогов, таких как “and”, “the” или “to” в английском языке. Удаление шумовых слов может исказить информацию и изменить контекст в конкретном предложении. Например, если мы проводим анализ тональности.

## Stemming (Стемминг)
Процесс обрезания конца или начала слов с целью удаления аффиксов (лексических дополнений к корню слова). Например, префикс “astro” в слове “astrobiology” и суффикс “ful” в слове “helpful”. Проблема в том, что аффиксы могут создавать или расширять новые формы одного и того же слова (так называемые формообразующие аффиксы) или даже сами создавать новые слова (так называемые словообразовательные аффиксы). 

Стемминг можно использовать для исправления орфографических ошибок в токенах. Стеммеры просты в использовании и работают очень быстро (они выполняют простые строковые операции), и если скорость и производительность имеют значение для вашей NLP-модели, то стеммеры, безусловно, являются хорошим выбором.

## Lemmatization (Лемматизация)
Призвана привести слово к его базовой форме и сгруппировать разные формы одного и того же слова. Например, глаголы в прошедшем времени заменяются на настоящее (например, “went” заменяется на “go”), а синонимы унифицируются (например, “best” заменяется на “good”), тем самым стандартизируя слова со значением, аналогичным их корню. Хотя лемматизация кажется тесно связанной с процессом стемминга, она использует другой подход для получения корневых форм слов.

Лемматизация преобразует слова в их словарную форму (известную как “лемма”), для чего требуются подробные словари, которые алгоритм может просматривать и связывать слова с соответствующими им леммами.

Лемматизация также принимает во внимание контекст слова для решения других проблем, таких как устранение неоднозначности, что означает, что она может различать идентичные слова, которые имеют разные значения в зависимости от конкретного контекста. 

**Недостатки:**

Лемматизация — гораздо более ресурсоемкая задача и требует большей вычислительной мощности, чем стемминг.

## Topic Modeling (Тематическое моделирование)
метод выявления скрытых структур в наборах текстов или документов. Он группирует тексты для обнаружения скрытых тем на основе их содержания, обрабатывая отдельные слова и присваивая им значения на основе их распределения. Этот метод основан на предположении, что каждый документ состоит из комбинации тем и что каждая тема (topic) состоит из набора слов, а это означает, что если мы сможем обнаружить эти скрытые темы, мы сможем раскрыть смысл наших текстов.

Латентное/Скрытое размещение Дирихле (Latent Dirichlet Allocation или LDA), вероятно, является наиболее часто используемым. Этот алгоритм работает как метод обучения без учителя, который обнаруживает различные темы, лежащие в основе набора документов. В методах обучения без учителя, подобных этому, нет выходной переменной, которая бы направляла процесс обучения - данные исследуются алгоритмами для поиска закономерностей. LDA находит группы связанных слов следующим образом:

1. Присваивая каждое слово случайной теме, где пользователь определяет количество тем, которые он хочет раскрыть. Вы не определяете сами темы (вы определяете только количество тем), и алгоритм сопоставляет все документы с темами таким образом, что слова в каждом документе в основном захватываются этими мнимыми темами.

2. Алгоритм итеративно перебирает каждое слово и переназначает слово теме, принимая во внимание вероятность того, что это слово принадлежит именно этой теме, и вероятность того, что документ был создан по этой теме. Эти вероятности вычисляются многократно, до приемлемой сходимости алгоритма.

В отличие от других алгоритмов кластеризации, таких как метод K-средних, которые выполняют жесткую кластеризацию (где темы не пересекаются), LDA присваивает каждому документу комбинацию тем, что означает, что каждый документ может быть описан одной или несколькими темами (например, Документ 1 описывается 70% темы A, 20% темы B и 10% темы C) и отражают более реалистичные результаты.


Тематическое моделирование чрезвычайно полезно для классификации текстов, создания рекомендательных систем (например, чтобы рекомендовать вам книги на основе уже прочитанных вами) или даже для выявления трендов в онлайн-публикациях.
