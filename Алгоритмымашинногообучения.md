# Алгоритмы машинного обучения

[Scikit-learn на русском](https://scikit-learn.ru/user_guide/)

Алгоритмы машинного обучения:
- Классическое обучение;
  - Обучение с учителем;
    -  Регрессия (Linear Regression, Polynomial Regression, Logistic Regression, Quantile Regression, Ridge Regression, Lasso Regression, ElasticNet Regression, Principal Component Regression, Partial Least Square Regression, Support Vector Regression, Ordinal Regression, Poisson Regression, Negative Binomial Regression, Quasi-Poisson Regression, Cox Regression);
    -  Классификация (kNN, Naive Bayes, [DecisionTrees](https://github.com/DenisPanchin/DS/blob/main/DecisionTree.ipynb), LogisticRegression)
  - Обучение без учителя:
    -  Кластеризация (DBSCAN, Agglomerative, k-Means, Mean-Shift, Fuzzy C-Means);
    -  Поиск правил (Euclat, Apriori, FP-Growth)
    -  Уменьшение размерности (t-SNE, PCA, LSA, SVD, LDA)
 - Обучение с подкреплением:
    -  Генетический Алгоритм;
    -  A3C;
    -  SARSA;
    -  Q-Learning;
    -  Deep Q-Network (DQN)
 - Нейросети и глубокое обучение:
  -  Convolutional Neural Networks (CNN);
      - DCNN;
  - Reccurent Neural Networks (RNN);
    - LSM;
    - LSTM;
    - GRU;
  - Generative Adversarial Networks (GAN);
  - Autoencoders;
    - seq2seq;
  - Перцептрон (MLP)  



### Линейная регрессия
Используется для моделирования взаимосвязи между двумя (или более) переменными. Различают два типа линейной регрессии  —  простую и множественную.

### SVM (Support Vector Machine, машина опорных векторов)
Алгоритм, который в основном используется в задачах классификации. Обычно на SVM-модель подаются помеченные обучающие данные, чтобы классифицировать новый текст. 

SVM является хорошим выбором, если имеется ограниченное количество образцов, а скорость является приоритетом. Именно поэтому SVM используется при работе с набором данных, содержащим несколько тысяч помеченных образцов.

### Дерево решений
Дерево решений  —  это модель, применяемая в планировании, статистике и машинном обучении. Она использует древовидную структуру решений/последствий для оценки возможных событий, связанных с определенной проблемой.

Алгоритм дерева решений можно использовать для решения задач как регрессии, так и классификации. В ML дерево решений часто применяют для построения модели, которая может предсказать класс или значение целевой переменной путем изучения правил дерева решений, выведенных из обучающих данных.

### Случайный лес
Случайный лес  —  это ансамбль из множества деревьев решений. Он сочетает в себе простоту дерева решений и гибкость, что приводит к повышению точности.

Чтобы создать случайный лес, сначала нужно создать “бутстрапный” набор данных. Бутстрап  —  это случайный выбор образцов из исходных данных (можно даже выбрать один и тот же образец несколько раз). Затем такой бутстрапный набор данных используется для создания дерева решений.

Этот метод также известен как “бэггинг”. Если повторить предыдущие шаги несколько раз, получим большое количество деревьев. Именно это разнообразие деревьев делает случайные леса более эффективными, чем одно дерево решений.

Если случайный лес используется для задачи классификации, модель выбирает режим предсказаний каждого дерева решений. Для задачи регрессии модель выбирает среднее значение результатов деревьев решений.

Гиперпараметры этой модели:

**"Max_depth"**: Этот гиперпараметр представляет максимальный уровень (глубину) каждого дерева в модели случайного леса. Более глубокое дерево показывает хорошие результаты и собирает много информации об обучающих данных, но плохо поддается обобщению для тестовых данных. По умолчанию в библиотеке Scikit-Learn это значение установлено в "None", что значит, такие деревья оставлены для полного расширения.

**"Max_features"**: Максимальное количество признаков, которое модели случайного леса разрешается опробовать при каждом разбиении. По умолчанию в Scikit-Learn это значение устанавливается равным квадратному корню из общего числа переменных в наборе данных.

**"N_estimators"**: Количество деревьев решений в лесу. По умолчанию в Scikit-Learn число оценщиков (estimators) равно 10.

**"Min_samples_leaf"**: Минимальное количество образцов, необходимое для нахождения в листовом узле каждого дерева. В Scikit-Learn значение по умолчанию равно 1.

**"Min_samples_split"**: Минимальное количество образцов, необходимое для разбиения внутреннего узла каждого дерева. В Scikit-Learn значение по умолчанию равно 2.

### Наивный байесовский классификатор
Наивный байесовский классификатор  —  это алгоритм, который использует условную вероятность для прогнозирования класса.

Наивный байесовский классификатор предполагает, что все признаки независимы друг от друга. Поскольку это не всегда так, перед выбором такого классификатора следует изучить данные.

Благодаря предположению о независимости признаков друг от друга, наивный байесовский классификатор является более быстрым, чем другие сложные алгоритмы, но менее точным.

Наивный байесовский классификатор применяется для прогнозирования погоды, выявления мошенничества и многого другого.

### Логистическая регрессия

Логистическая регрессия  —  это алгоритм, который обычно применяется для решения задач бинарной классификации. Из этого следует, что логистическую регрессию можно использовать для прогнозирования оттока клиентов, а также для определения того, является ли письмо спамом или нет.

Логистическая регрессия основана на логистической функции (она же сигмоидная функция), которая принимает значение и присваивает ему вероятность от 0 до 1.

### Кластеризация (k-Means)
При работе с большими наборами данных scikit-learn требует много времени для обучения и прогнозирования. Чтобы ускорить KMeans, используйте Faiss от Facebook AI Research. Он обеспечивает более быстрый поиск ближайшего соседа и кластеризацию. Faiss использует «инвертированный индекс», оптимизированную структуру данных для хранения и индексации точек данных. Это делает выполнение кластеризации чрезвычайно эффективной. Кроме того, Faiss обеспечивает распараллеливание и поддержку GPU, что еще больше повышает производительность его алгоритмов кластеризации.
```python
import faiss
kmeans = faiss.Kmeans(d=1024, k=8)
kmeans.train(X_train)
```

